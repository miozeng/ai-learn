{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic回归算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic 回归虽然名字叫回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线建立回归公式，以此进行分类。\n",
    "具体请参考逻辑回归.docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要步骤: \n",
    "- 初始化模型的参数\n",
    "- 通过缩小损失函数学习到参数\n",
    "- 通过学习得到的参数做出预测\n",
    "- 分析结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1 :获取模型数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data_set():\n",
    "    \"\"\"\n",
    "    创建样本数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_train = [[-0.017612, 14.053064], [-1.395634, 4.662541], [-0.752157, 6.538620], \n",
    "                [-1.322371, 7.152853], [0.423363, 11.054677],[0.406704,7.067335],\n",
    "                [0.667394,12.741452],[-2.460150,6.866805],[0.569411,9.548755],\n",
    "               [-1.693453,-0.557540],[1.985298,3.230619],[-1.78187,9.097953]]\n",
    "    Y_train = [0,1,0,0,0,1,0,1,0,1,1,0]\n",
    "    \n",
    "    X_test = [[-0.346811,-1.678730], [-2.124484,2.672471], [1.217916,9.597015]]\n",
    "    Y_test = [1,1,0]\n",
    "    return mat(X_train), mat(Y_train).transpose() , mat(X_test), mat(Y_test).transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inX):\n",
    "    # return 1.0 / (1 + exp(-inX))\n",
    "\n",
    "    # Tanh是Sigmoid的变形，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好。\n",
    "    return 2 * 1.0/(1+exp(-2*inX)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2 :初始化参数为零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step3:LOOP获取 根据梯度下降获取参数值等\n",
    "    1.获取输入参数X\n",
    "    2.计算A=sigm(w.T*X+b)=(a0,a1,....am)\n",
    "    3.计算损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.定义梯度下降的方法获取cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    #print(w.T.shape)\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T, X)+b)    \n",
    "    # compute activation\n",
    "    cost = -(1.0/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                 # compute cost\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dz=A-Y\n",
    "    dw = (1.0/m)*np.dot(X,dz.T)\n",
    "    db = (1.0/m)*np.sum(dz)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    " \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.使用梯度下降发优化w和b使得cost最小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation \n",
    "        print(w.shape)\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        print(dw)\n",
    "        print(dw.shape)\n",
    "        print(db.shape)\n",
    "        print(w.shape)\n",
    "        # update rule (≈ 2 lines of code)\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step4:预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(w, b, X):\n",
    " \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0,i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### step5:合并所有的方法到一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # m个数，n特征数\n",
    "    X_train, Y_train, X_test, Y_test = create_data_set()\n",
    "  \n",
    " \n",
    "    model(X_train, Y_train, X_test, Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.linear_model.LogisticRegression(penalty='l2', \n",
    "          dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "          intercept_scaling=1, class_weight=None, \n",
    "          random_state=None, solver='liblinear', max_iter=100, \n",
    "          multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "          \n",
    "penalty='l2' : 字符串‘l1’或‘l2’,默认‘l2’。用来指定惩罚的基准（正则化参数）。\n",
    "\n",
    "dual=False : 对偶或者原始方法。Dual只适用于正则化相为l2的‘liblinear’的情况，通常样本数大于特征数的情况下，默认为False。\n",
    "\n",
    "C=1.0 : C为正则化系数λ的倒数，必须为正数，默认为1。和SVM中的C一样，值越小，代表正则化越强。\n",
    "\n",
    "fit_intercept=True : 是否存在截距，默认存在。\n",
    "\n",
    "intercept_scaling=1 : 仅在正则化项为‘liblinear’，且fit_intercept设置为True时有用。\n",
    "\n",
    "。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression类的常用方法\n",
    "fit(X, y, sample_weight=None)\n",
    "拟合模型，用来训练LR分类器，其中X是训练样本，y是对应的标记向量\n",
    "返回对象，self。 \n",
    "\n",
    "fit_transform(X, y=None, **fit_params)\n",
    "fit与transform的结合，先fit后transform。返回X_new:numpy矩阵。\n",
    "\n",
    "predict(X)\n",
    "用来预测样本，也就是分类，X是测试集。返回array。\n",
    "\n",
    "predict_proba(X)\n",
    "输出分类概率。返回每种类别的概率，按照分类类别顺序给出。如果是多分类问题，multi_class=\"multinomial\"，则会给出样本对于每种类别的概率。\n",
    "返回array-like。\n",
    "\n",
    "score(X, y, sample_weight=None)\n",
    "返回给定测试集合的平均准确率（mean accuracy），浮点型数值。\n",
    "对于多个分类返回，则返回每个类别的准确率组成的哈希矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coffee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coffee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Coffee\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression(penalty='l1')\n",
    "model2.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.93780491e-01 6.21950927e-03]\n",
      " [2.69531453e-02 9.73046855e-01]\n",
      " [1.33340841e-03 9.98666592e-01]\n",
      " [1.48258938e-01 8.51741062e-01]\n",
      " [5.37251923e-05 9.99946275e-01]\n",
      " [2.24632247e-03 9.97753678e-01]\n",
      " [6.00957992e-03 9.93990420e-01]\n",
      " [9.94317364e-04 9.99005683e-01]\n",
      " [3.11987989e-02 9.68801201e-01]\n",
      " [1.30612216e-04 9.99869388e-01]\n",
      " [3.50587072e-01 6.49412928e-01]\n",
      " [1.40108350e-01 8.59891650e-01]\n",
      " [3.17754589e-03 9.96822454e-01]\n",
      " [7.47538261e-01 2.52461739e-01]\n",
      " [1.58297053e-01 8.41702947e-01]\n",
      " [9.94214371e-01 5.78562895e-03]\n",
      " [1.92716007e-02 9.80728399e-01]\n",
      " [9.99999999e-01 5.83426795e-10]\n",
      " [9.99043138e-01 9.56861542e-04]\n",
      " [1.00000000e+00 5.83130355e-13]\n",
      " [9.99978985e-01 2.10146155e-05]\n",
      " [9.30516928e-01 6.94830722e-02]\n",
      " [1.08558860e-03 9.98914411e-01]\n",
      " [8.26679714e-03 9.91733203e-01]\n",
      " [9.95566755e-01 4.43324490e-03]\n",
      " [7.17184114e-03 9.92828159e-01]\n",
      " [1.11211654e-03 9.98887883e-01]\n",
      " [8.12148287e-01 1.87851713e-01]\n",
      " [2.39105256e-03 9.97608947e-01]\n",
      " [1.00000000e+00 2.29600568e-11]\n",
      " [2.20276598e-04 9.99779723e-01]\n",
      " [9.99999990e-01 1.00029236e-08]\n",
      " [4.05747573e-01 5.94252427e-01]\n",
      " [9.99500696e-01 4.99304057e-04]\n",
      " [1.02771941e-04 9.99897228e-01]\n",
      " [9.99000849e-01 9.99151069e-04]\n",
      " [6.36202004e-02 9.36379800e-01]\n",
      " [9.99998813e-01 1.18684110e-06]\n",
      " [3.47067528e-03 9.96529325e-01]\n",
      " [9.99806661e-01 1.93338722e-04]\n",
      " [9.55639803e-01 4.43601972e-02]\n",
      " [3.74052278e-03 9.96259477e-01]\n",
      " [9.97936596e-01 2.06340408e-03]\n",
      " [5.49997033e-04 9.99450003e-01]\n",
      " [9.46899536e-01 5.31004637e-02]\n",
      " [1.00000000e+00 3.73807869e-13]\n",
      " [1.81753399e-04 9.99818247e-01]\n",
      " [1.92706802e-02 9.80729320e-01]\n",
      " [7.10362415e-04 9.99289638e-01]\n",
      " [9.99777966e-01 2.22033809e-04]\n",
      " [9.99997897e-01 2.10259729e-06]\n",
      " [6.63676050e-01 3.36323950e-01]\n",
      " [9.99999818e-01 1.82284848e-07]\n",
      " [3.17749965e-03 9.96822500e-01]\n",
      " [6.62792751e-03 9.93372072e-01]\n",
      " [6.04025421e-04 9.99395975e-01]\n",
      " [1.10793268e-02 9.88920673e-01]\n",
      " [2.09053825e-02 9.79094618e-01]\n",
      " [1.17941675e-02 9.88205833e-01]\n",
      " [1.00000000e+00 5.76075742e-18]\n",
      " [9.87518825e-01 1.24811755e-02]\n",
      " [9.99961110e-01 3.88901055e-05]\n",
      " [1.55778462e-03 9.98442215e-01]\n",
      " [3.30382762e-03 9.96696172e-01]\n",
      " [9.99999966e-01 3.43654100e-08]\n",
      " [1.13728077e-01 8.86271923e-01]\n",
      " [1.00000000e+00 3.60188730e-19]\n",
      " [9.99997972e-01 2.02800472e-06]\n",
      " [9.99999983e-01 1.66734256e-08]\n",
      " [6.42443987e-03 9.93575560e-01]\n",
      " [5.81415779e-01 4.18584221e-01]\n",
      " [9.99995427e-01 4.57301154e-06]\n",
      " [1.95211727e-03 9.98047883e-01]\n",
      " [1.63464747e-01 8.36535253e-01]\n",
      " [9.99999589e-01 4.11028845e-07]\n",
      " [2.01311472e-02 9.79868853e-01]\n",
      " [1.54560892e-03 9.98454391e-01]\n",
      " [5.70335179e-03 9.94296648e-01]\n",
      " [3.92394171e-03 9.96076058e-01]\n",
      " [4.74999296e-03 9.95250007e-01]\n",
      " [9.98976242e-01 1.02375832e-03]\n",
      " [1.00000000e+00 1.56530817e-10]\n",
      " [9.99996944e-01 3.05574438e-06]\n",
      " [2.19881179e-04 9.99780119e-01]\n",
      " [9.55894877e-01 4.41051230e-02]\n",
      " [4.65681772e-04 9.99534318e-01]\n",
      " [1.82346508e-03 9.98176535e-01]\n",
      " [1.09192322e-03 9.98908077e-01]\n",
      " [9.99992404e-01 7.59556570e-06]\n",
      " [1.00000000e+00 4.50975160e-12]\n",
      " [1.94559504e-04 9.99805440e-01]\n",
      " [7.84849239e-01 2.15150761e-01]\n",
      " [6.24144944e-01 3.75855056e-01]\n",
      " [9.99926405e-01 7.35949220e-05]\n",
      " [1.22774391e-02 9.87722561e-01]\n",
      " [9.61136752e-03 9.90388632e-01]\n",
      " [1.00000000e+00 5.38781159e-16]\n",
      " [2.40916678e-01 7.59083322e-01]\n",
      " [2.67204827e-02 9.73279517e-01]\n",
      " [5.81005786e-03 9.94189942e-01]\n",
      " [1.23798483e-03 9.98762015e-01]\n",
      " [3.51204681e-03 9.96487953e-01]\n",
      " [5.01471735e-03 9.94985283e-01]\n",
      " [9.48945199e-02 9.05105480e-01]\n",
      " [9.99999923e-01 7.73458503e-08]\n",
      " [1.10839985e-03 9.98891600e-01]\n",
      " [9.99989345e-01 1.06551959e-05]\n",
      " [1.11886101e-01 8.88113899e-01]\n",
      " [8.24400764e-01 1.75599236e-01]\n",
      " [8.44113365e-01 1.55886635e-01]\n",
      " [1.36529025e-02 9.86347098e-01]\n",
      " [9.99763492e-01 2.36508323e-04]\n",
      " [9.99995606e-01 4.39434188e-06]\n",
      " [9.74210420e-02 9.02578958e-01]]\n"
     ]
    }
   ],
   "source": [
    "prepro = model2.predict_proba(X_test)\n",
    "print(prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
