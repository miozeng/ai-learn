传统机器学习算法主要包括以下五类：

    回归：建立一个回归方程来预测目标值，用于连续型分布预测
    分类：给定大量带标签的数据，计算出未知标签样本的标签取值
    聚类：将不带标签的数据根据距离聚集成不同的簇，每一簇数据有共同的特征
    关联分析：计算出数据之间的频繁项集合
    降维：原高维空间中的数据点映射到低维度的空间中

1.[线性回归](线性回归.ipynb)：找到一条直线来预测目标值

    优点：结果易于理解，计算不复杂
    缺点：对非线性的数据拟合不好
    应用：根据房屋面积和售价的关系，求出回归方程，则可以预测给定房屋面积时的售价。电影票房预测等


2. [逻辑回归](Logistic回归算法.ipynb)：找到一条直线来分类数据
逻辑回归虽然名字叫回归，却是属于分类算法，是通过Sigmoid函数将线性函数的结果映射到Sigmoid函数中，预估事件出现的概率并分类。

  优点：

    实现简单，广泛的应用于工业问题上；
    分类时计算量非常小，速度很快，存储资源低；
    便利的观测样本概率分数；
    对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；
    计算代价不高，易于理解和实现；
缺点：

    当特征空间很大时，逻辑回归的性能不是很好；
    容易欠拟合，一般准确度不太高
    不能很好地处理大量多类特征或变量；
    只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；
    对于非线性特征，需要进行转换；
    logistic回归应用领域：
    
应用

    用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。
    Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。
    信用评估
    测量市场营销的成功度
    预测某个产品的收益
    特定的某天是否会发生地震

3. [K-近邻](K-近邻.ipynb)：用距离度量最相邻的分类标签
可以使用K近邻算法，其工作原理如下：

    计算样本数据中的点与当前点之间的距离
    算法提取样本最相似数据(最近邻)的分类标签
    确定前k个点所在类别的出现频率. 一般只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数
    返回前k个点所出现频率最高的类别作为当前点的预测分类
    
KNN算法的优点

    理论成熟，思想简单，既可以用来做分类也可以用来做回归；
    可用于非线性分类；
    训练时间复杂度为O(n)；
    对数据没有假设，准确度高，对outlier不敏感；
    KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练；
    KNN理论简单，容易实现；
    
缺点

    样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差；
    需要大量内存；
    对于样本容量大的数据集计算量比较大（体现在距离计算上）；
    样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多；
    KNN每一次分类都会重新进行一次全局运算；
    k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择；
    
KNN算法应用领域

    文本分类、模式识别、聚类分析，多分类领域

4.[朴素贝叶斯](朴素贝叶斯.ipynb)：朴素贝叶斯（NB）是一种基于贝叶斯定理和特征条件独立假设的分类方法。本质上朴素贝叶斯模型就是一个概率表，其通过训练数据更新这张表中的概率。为了预测一个新的观察值，朴素贝叶斯算法就是根据样本的特征值在概率表中寻找最大概率的那个类别。

之所以称之为「朴素」，是因为该算法的核心就是特征条件独立性假设（每一个特征之间相互独立），而这一假设在现实世界中基本是不现实的。

优点：

        朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。
        对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已；
        对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）；
        对缺失数据不太敏感，算法也比较简单，常用于文本分类；
        朴素贝叶斯对结果解释容易理解；
缺点：

    需要计算先验概率；
    分类决策存在错误率；
    对输入数据的表达形式很敏感；
    由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好；
    
朴素贝叶斯应用领域

    欺诈检测中使用较多
    一封电子邮件是否是垃圾邮件（垃圾文本过滤）
    一篇文章应该分到科技、政治，还是体育类（文本分类）
    一段文字表达的是积极的情绪还是消极的情绪？（情感判别）
    人脸识别

5. [决策树](决策树.ipynb)：构造一棵熵值下降最快的分类树
策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶结点代表一种类别。采用的是自顶向下的递归方法，选择信息增益最大的特征作为当前的分裂特征。

决策树的优点：

    （1）具有可读性，如果给定一个模型，那么过呢据所产生的决策树很容易推理出相应的逻辑表达。

    （2）分类速度快，能在相对短的时间内能够对大型数据源做出可行且效果良好的结果。

决策树的缺点：

    （1）对未知的测试数据未必有好的分类、泛化能力，即可能发生过拟合现象，此时可采用剪枝或随机森林

决策树可以应于：用户分级评估、贷款风险评估、选股、投标决策，企业投资决策等，由于决策树很好的分析能力，在决策过程应用较多。

5.1[随机森林](随机森林.ipynb)集成学习是将多个模型进行组合来解决单一的预测问题。它的原理是生成多个分类器模型，各自独立地学习并作出预测。这些预测最后结合起来得到预测结果，因此和单独分类器的结果相比，结果一样或更好。

优点： 

    1） 每棵树都选择部分样本及部分特征，一定程度避免过拟合； 
    2） 每棵树随机选择样本并随机选择特征，使得具有很好的抗噪能力，性能稳定； 
    3） 能处理很高维度的数据，并且不用做特征选择； 
    4） 适合并行计算； 
    5） 实现比较简单； 
    
缺点： 

    1） 参数较复杂； 
    2） 模型训练和预测都比较慢。
    
随机森林主要用途

    1.特征选择 
      你可以检查变量在每棵树中表现的是最佳还是最糟糕。当一些树使用一个变量，而其他的不使用这个变量，你就可以对比信息的丢失或增加。
    2.分类
      它可以被用于为多个可能目标类别做预测，它也可以在调整后输出概率。
    3.回归

6.[支持向量机（SVM）](svm.ipynb)：构造超平面，分类非线性数据
不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为支持向量。

优点：SVM 能对非线性决策边界建模，并且有许多可选的核函数形式。SVM 同样面对过拟合有相当大的鲁棒性，这一点在高维空间中尤其突出。

缺点：然而，SVM 是内存密集型算法，由于选择正确的核函数是很重要的，所以其很难调参，也不能扩展到较大的数据集中。目前在工业界中，随机森林通常优于支持向量机算法。

SVM的应用非常广泛，可以应用于垃圾邮件识别、手写识别、文本分类、图像识别、选股等。（主要二分类领域，毕竟常规SVM只能解决二分类问题）

7. [K-means](K-Means.ipynb)：计算质心，聚类无标签数据
K-Means算法是一种常见的聚类算法，其基本步骤为：

    随机生成k个初始点作为质心；
    将数据集中的数据按照距离质心的远近分到各个簇中；
    将各个簇中的数据求平均值，作为新的质心，重复上一步，直到所有的簇不再改变。 两个分类间隔越远，则聚类效果越好。
    K-means算法的一个案例是：客户价值细分，精准投资。
    以航空公司为例，因为业务竞争激烈，企业营销焦点从产品中心转为客户中心；建立合理的客户价值评估模型，进行客户分类，进行精准营销，是解决问题的关键。

优点:

    属于无监督学习，无须准备训练集
    原理简单，实现起来较为容易
    结果可解释性较好
缺点:

    需手动设置k值。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性
    可能收敛到局部最小值, 在大规模数据集上收敛较慢
    对于异常点、离群点敏感
    
8. [关联分析](关联分析.ipynb)：挖掘啤酒与尿布（频繁项集）的关联规则
比较常见的一种关联算法是FP-growth算法。
算法中几个相关的概念：

    频繁项集：在数据库中大量频繁出现的数据集合。例如购物单数据中{'啤酒'}、{'尿布'}、{'啤酒', '尿布'}出现的次数都比较多。
    关联规则：由集合 A，可以在某置信度下推出集合 B。即如果 A 发生了，那么 B 也很有可能会发生。例如购买了{'尿布'}的人很可能会购买{'啤酒'}。
    支持度：指某频繁项集在整个数据集中的比例。假设数据集有 10 条记录，包含{'啤酒', '尿布'}的有 5 条记录，那么{'啤酒', '尿布'}的支持度就是 5/10 = 0.5。
    置信度：有关联规则如{'尿布'} -> {'啤酒'}，它的置信度为 {'尿布'} -> {'啤酒'}

9. [PCA降维](PCA降维.ipynb)：减少数据维度，降低数据复杂度
降维是指将原高维空间中的数据点映射到低维度的空间中。因为高维特征的数目巨大，距离计算困难，分类器的性能会随着特征数的增加而下降；减少高维的冗余信息所造成的误差,可以提高识别的精度。



sklearn 中文文档
https://sklearn.apachecn.org/#/
主要参考网站
https://github.com/apachecn/AiLearning
和吴恩达的课程







