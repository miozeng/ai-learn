传统机器学习算法主要包括以下五类：

回归：建立一个回归方程来预测目标值，用于连续型分布预测
分类：给定大量带标签的数据，计算出未知标签样本的标签取值
聚类：将不带标签的数据根据距离聚集成不同的簇，每一簇数据有共同的特征
关联分析：计算出数据之间的频繁项集合
降维：原高维空间中的数据点映射到低维度的空间中

1.线性回归(线性回归.ipynb)：找到一条直线来预测目标值
应用：根据房屋面积和售价的关系，求出回归方程，则可以预测给定房屋面积时的售价。电影票房预测等


2. 逻辑回归(Logistic回归算法.ipynb)：找到一条直线来分类数据
逻辑回归虽然名字叫回归，却是属于分类算法，是通过Sigmoid函数将线性函数的结果映射到Sigmoid函数中，预估事件出现的概率并分类。


3. K-近邻(K-近邻.ipynb)：用距离度量最相邻的分类标签
可以使用K近邻算法，其工作原理如下：

计算样本数据中的点与当前点之间的距离
算法提取样本最相似数据(最近邻)的分类标签
确定前k个点所在类别的出现频率. 一般只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数
返回前k个点所出现频率最高的类别作为当前点的预测分类


4. 朴素贝叶斯：选择后验概率最大的类为分类标签
朴素贝叶斯的主要应用有文本分类、垃圾文本过滤，情感判别，多分类实时预测等


5. 决策树(决策树.ipynb)：构造一棵熵值下降最快的分类树
策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶结点代表一种类别。采用的是自顶向下的递归方法，选择信息增益最大的特征作为当前的分裂特征。

决策树可以应于：用户分级评估、贷款风险评估、选股、投标决策等。

6. 支持向量机（SVM）：构造超平面，分类非线性数据
不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为支持向量。

SVM的应用非常广泛，可以应用于垃圾邮件识别、手写识别、文本分类、选股等。

7. K-means：计算质心，聚类无标签数据
K-Means算法是一种常见的聚类算法，其基本步骤为：

随机生成k个初始点作为质心；
将数据集中的数据按照距离质心的远近分到各个簇中；
将各个簇中的数据求平均值，作为新的质心，重复上一步，直到所有的簇不再改变。 两个分类间隔越远，则聚类效果越好。
K-means算法的一个案例是：客户价值细分，精准投资。
以航空公司为例，因为业务竞争激烈，企业营销焦点从产品中心转为客户中心；建立合理的客户价值评估模型，进行客户分类，进行精准营销，是解决问题的关键。


8. 关联分析：挖掘啤酒与尿布（频繁项集）的关联规则
比较常见的一种关联算法是FP-growth算法。
算法中几个相关的概念：

频繁项集：在数据库中大量频繁出现的数据集合。例如购物单数据中{'啤酒'}、{'尿布'}、{'啤酒', '尿布'}出现的次数都比较多。
关联规则：由集合 A，可以在某置信度下推出集合 B。即如果 A 发生了，那么 B 也很有可能会发生。例如购买了{'尿布'}的人很可能会购买{'啤酒'}。
支持度：指某频繁项集在整个数据集中的比例。假设数据集有 10 条记录，包含{'啤酒', '尿布'}的有 5 条记录，那么{'啤酒', '尿布'}的支持度就是 5/10 = 0.5。
置信度：有关联规则如{'尿布'} -> {'啤酒'}，它的置信度为 {'尿布'} -> {'啤酒'}

9. PCA降维：减少数据维度，降低数据复杂度
降维是指将原高维空间中的数据点映射到低维度的空间中。因为高维特征的数目巨大，距离计算困难，分类器的性能会随着特征数的增加而下降；减少高维的冗余信息所造成的误差,可以提高识别的精度。










