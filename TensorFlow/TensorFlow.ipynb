{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "from sklearn import preprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "testdata = pd.read_csv('../data/dataannalysis/happiness_test_abbr.csv')\n",
    "traindata = pd.read_csv('../data/dataannalysis/happiness_train_abbr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#异常数据处理\n",
    "traindata = traindata[traindata['happiness'] != -8]\n",
    "traindata = traindata[traindata['depression'] != -8]\n",
    "traindata = traindata[traindata['class'] != -8]\n",
    "traindata = traindata[traindata['health'] != -8]\n",
    "traindata = traindata[traindata['equity'] != -8]\n",
    "traindata = traindata[traindata['family_status'] != -8]\n",
    "traindata = traindata[traindata['health_problem'] != -8]\n",
    "traindata = traindata[traindata['relax'] != -8]\n",
    "traindata = traindata[traindata['learn'] != -8]\n",
    "traindata['view'].replace(-8,3,inplace = True)\n",
    "traindata = traindata[traindata['socialize'] != -8]\n",
    "traindata = traindata[traindata['socialize'] != -1]\n",
    "traindata = traindata[traindata['socialize'] != -2]\n",
    "traindata = traindata[traindata['socialize'] != -3]\n",
    "traindata = traindata[traindata['socialize'] != 50]\n",
    "traindata = traindata[traindata['status_3_before'] != -8]\n",
    "traindata = traindata[traindata['status_peer'] != -8]\n",
    "traindata['inc_ability'].replace(-8,3,inplace = True)\n",
    "\n",
    "#数据重新处理\n",
    "traindata['marital'].replace(2,1,inplace = True)\n",
    "traindata['marital'].replace(3,1,inplace = True)\n",
    "traindata['marital'].replace(4,1,inplace = True)\n",
    "traindata['marital'].replace(7,1,inplace = True)\n",
    "traindata['marital'].replace(6,2,inplace = True)\n",
    "traindata['marital'].replace(5,3,inplace = True)\n",
    "\n",
    "bins = [-100,0,100000,300000,500000,1000000,10000000] \n",
    "traindata['income'] = pd.cut(traindata['income'],bins, labels=['1','2', '3', '4', '5', '6'])\n",
    "\n",
    "\n",
    "\n",
    "avg_income=traindata['family_income']/traindata['family_m']\n",
    "traindata['avg_income'] = avg_income\n",
    "bins = [-100,0,20000,50000,100000,1000000,10000000] \n",
    "traindata['avg_income'] = pd.cut(traindata['avg_income'],bins, labels=['1','2', '3', '4', '5', '6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindata['avg_income'].('unknown', inplace=True)\n",
    "traindata['avg_income'] = traindata['avg_income'].cat.add_categories(['0']);\n",
    "traindata['avg_income'].fillna('0', inplace=True)\n",
    "\n",
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "le.fit(['1','2', '3', '4', '5', '6', '0']) \n",
    "traindata['avg_income']  = le.transform(traindata['avg_income'] ) \n",
    "\n",
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "le.fit(['1','2', '3', '4', '5', '6']) \n",
    "traindata['income']  = le.transform(traindata['income'] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindatafinal = traindata[['happiness','depression','class','health','equity','family_status','health_problem','relax','learn','view','socialize','status_peer','status_3_before','inc_ability','car','avg_income','edu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = traindatafinal[:7000]\n",
    "test_data = traindatafinal[7000:]\n",
    " \n",
    "train_data_X = train_data.drop(['happiness'],axis=1)\n",
    "train_data_Y = train_data['happiness']\n",
    "test_data_X = test_data.drop(['happiness'],axis=1)\n",
    "test_data_Y = test_data['happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 16)\n",
      "(7000,)\n",
      "(718, 16)\n",
      "(718,)\n",
      "(7000, 5)\n",
      "(718, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_X.shape)\n",
    "print(train_data_Y.shape)\n",
    "print(test_data_X.shape)\n",
    "print(test_data_Y.shape)\n",
    "#train_data_X_ =train_data_X.T\n",
    "#test_data_X_ = test_data_X.T\n",
    "#print(train_data_X_.shape)\n",
    "#print(test_data_X_.shape)\n",
    "#train_data_Y_ = train_data_Y.reshape((1, train_data_Y.shape[0]))\n",
    "#test_data_tY_ = test_data_Y.reshape((1, test_data_Y.shape[0]))\n",
    "test_data_Y_ = pd.get_dummies(test_data_Y)\n",
    "#train_data_Y_ = pd.get_dummies(train_data_Y).T #get_dummies one hot\n",
    "train_data_Y_ = pd.get_dummies(train_data_Y)\n",
    "\n",
    "print(train_data_Y_.shape)\n",
    "print(test_data_Y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depression</th>\n",
       "      <th>class</th>\n",
       "      <th>health</th>\n",
       "      <th>equity</th>\n",
       "      <th>family_status</th>\n",
       "      <th>health_problem</th>\n",
       "      <th>relax</th>\n",
       "      <th>learn</th>\n",
       "      <th>view</th>\n",
       "      <th>socialize</th>\n",
       "      <th>status_peer</th>\n",
       "      <th>status_3_before</th>\n",
       "      <th>inc_ability</th>\n",
       "      <th>car</th>\n",
       "      <th>avg_income</th>\n",
       "      <th>edu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.258736</td>\n",
       "      <td>-0.816038</td>\n",
       "      <td>-0.583207</td>\n",
       "      <td>-0.193497</td>\n",
       "      <td>-0.928463</td>\n",
       "      <td>-1.759826</td>\n",
       "      <td>0.730827</td>\n",
       "      <td>0.973896</td>\n",
       "      <td>0.564666</td>\n",
       "      <td>-0.773327</td>\n",
       "      <td>1.302760</td>\n",
       "      <td>0.388394</td>\n",
       "      <td>1.026235</td>\n",
       "      <td>0.397537</td>\n",
       "      <td>0.783352</td>\n",
       "      <td>1.948324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.919008</td>\n",
       "      <td>1.032991</td>\n",
       "      <td>1.298972</td>\n",
       "      <td>-0.193497</td>\n",
       "      <td>1.875956</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.730827</td>\n",
       "      <td>0.973896</td>\n",
       "      <td>0.564666</td>\n",
       "      <td>-0.773327</td>\n",
       "      <td>-2.349351</td>\n",
       "      <td>-1.227028</td>\n",
       "      <td>-0.722462</td>\n",
       "      <td>0.397537</td>\n",
       "      <td>-0.427660</td>\n",
       "      <td>2.267199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.258736</td>\n",
       "      <td>0.416648</td>\n",
       "      <td>0.357883</td>\n",
       "      <td>0.804647</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.730827</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>0.564666</td>\n",
       "      <td>0.186816</td>\n",
       "      <td>-0.523295</td>\n",
       "      <td>-1.227028</td>\n",
       "      <td>-0.722462</td>\n",
       "      <td>0.397537</td>\n",
       "      <td>-0.427660</td>\n",
       "      <td>-0.283798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.169864</td>\n",
       "      <td>0.416648</td>\n",
       "      <td>0.357883</td>\n",
       "      <td>0.804647</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.730827</td>\n",
       "      <td>1.900533</td>\n",
       "      <td>-0.827116</td>\n",
       "      <td>-0.773327</td>\n",
       "      <td>-0.523295</td>\n",
       "      <td>-1.227028</td>\n",
       "      <td>-0.722462</td>\n",
       "      <td>-1.857533</td>\n",
       "      <td>-0.427660</td>\n",
       "      <td>-0.602673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.919008</td>\n",
       "      <td>-2.048723</td>\n",
       "      <td>1.298972</td>\n",
       "      <td>-1.191642</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>1.077078</td>\n",
       "      <td>-0.331642</td>\n",
       "      <td>1.900533</td>\n",
       "      <td>-0.827116</td>\n",
       "      <td>1.146960</td>\n",
       "      <td>1.302760</td>\n",
       "      <td>0.388394</td>\n",
       "      <td>1.026235</td>\n",
       "      <td>-1.857533</td>\n",
       "      <td>-1.638673</td>\n",
       "      <td>2.267199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depression     class    health    equity  family_status  health_problem  \\\n",
       "0    1.258736 -0.816038 -0.583207 -0.193497      -0.928463       -1.759826   \n",
       "1   -0.919008  1.032991  1.298972 -0.193497       1.875956        0.131443   \n",
       "2    1.258736  0.416648  0.357883  0.804647       0.473747        0.131443   \n",
       "3    0.169864  0.416648  0.357883  0.804647       0.473747        0.131443   \n",
       "4   -0.919008 -2.048723  1.298972 -1.191642       0.473747        1.077078   \n",
       "\n",
       "      relax     learn      view  socialize  status_peer  status_3_before  \\\n",
       "0  0.730827  0.973896  0.564666  -0.773327     1.302760         0.388394   \n",
       "1  0.730827  0.973896  0.564666  -0.773327    -2.349351        -1.227028   \n",
       "2  0.730827  0.047258  0.564666   0.186816    -0.523295        -1.227028   \n",
       "3  0.730827  1.900533 -0.827116  -0.773327    -0.523295        -1.227028   \n",
       "4 -0.331642  1.900533 -0.827116   1.146960     1.302760         0.388394   \n",
       "\n",
       "   inc_ability       car  avg_income       edu  \n",
       "0     1.026235  0.397537    0.783352  1.948324  \n",
       "1    -0.722462  0.397537   -0.427660  2.267199  \n",
       "2    -0.722462  0.397537   -0.427660 -0.283798  \n",
       "3    -0.722462 -1.857533   -0.427660 -0.602673  \n",
       "4     1.026235 -1.857533   -1.638673  2.267199  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X =train_data_X.astype(np.float32)\n",
    "x1=train_data_X-np.mean(train_data_X,axis=0)\n",
    "normal_train_data_X=x1/np.std(x1,axis=0)\n",
    "##normal_train_data_X=tf.nn.local_response_normalization(train_data_X,1,0,1,1) \n",
    "normal_train_data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#创建一个神经网络层\n",
    "def add_layer(input,in_size,out_size,activation_function=None):\n",
    "    \"\"\"\n",
    "    :param input: 数据输入\n",
    "    :param in_size: 输入大小\n",
    "    :param out_size: 输出大小\n",
    "    :param activation_function: 激活函数（默认没有）\n",
    "    :return:output：数据输出\n",
    "    \"\"\"\n",
    "    Weight=tf.Variable(tf.random_normal([in_size,out_size]) )\n",
    "    biases=tf.Variable(tf.zeros([1,out_size]) +0.1 )\n",
    "    W_mul_x_plus_b=tf.matmul(input,Weight) + biases\n",
    "    #根据是否有激活函数\n",
    "    if activation_function == None:\n",
    "        output=W_mul_x_plus_b\n",
    "    else:\n",
    "        output=activation_function(W_mul_x_plus_b)\n",
    "    return output\n",
    "lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027020800000000.0\n",
      "1188.3124\n",
      "0.5760737\n",
      "0.5760737\n",
      "0.5760737\n",
      "0.5760737\n",
      "0.5760737\n",
      "0.5760737\n",
      "0.5760737\n",
      "0.5760737\n"
     ]
    }
   ],
   "source": [
    "lambd =0.01\n",
    "regularizer = tf.contrib.layers.l2_regularizer(lambd)\n",
    "#定义输入数据\n",
    "xs=tf.placeholder(tf.float32,[None,16])\n",
    "ys=tf.placeholder(tf.float32,[None,5])\n",
    " \n",
    "#定义一个隐藏层\n",
    "hidden_layer1=add_layer(xs,16,10,activation_function=tf.nn.relu)\n",
    "hidden_layer2=add_layer(hidden_layer1,10,7,activation_function=tf.nn.relu)\n",
    "#定义一个隐藏层\n",
    "#hidden_layer1=add_layer(xs,16,10,activation_function=tf.nn.relu)\n",
    "#定义一个输出层\n",
    "prediction=add_layer(hidden_layer2,7,5,activation_function=None)\n",
    " \n",
    "#求解神经网络参数\n",
    "#1.定义损失函数\n",
    "loss=tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction) ,reduction_indices=[1] ))\n",
    "#2.定义训练过程\n",
    "train_step=tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    " \n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "#3.进行训练\n",
    "for i in range(1000):\n",
    "    sess.run(train_step,feed_dict={xs:train_data_X,ys:train_data_Y_})\n",
    "    if i%100==0:\n",
    "        print(sess.run(loss,feed_dict={xs:train_data_X,ys:train_data_Y_} )  )\n",
    " \n",
    "#关闭sess\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layers(input_tensor,regularizer):\n",
    "    HINDENN1 = 7\n",
    "    HINDENN2 = 5\n",
    "    with tf.name_scope(\"full-connect-layer\"):\n",
    "        fc1 = tf.layers.dense(input_tensor, HINDENN1, activation=tf.nn.elu,\\\n",
    "            kernel_regularizer=regularizer)\n",
    "        fc2 = tf.layers.dense(fc1, HINDENN2, activation=tf.nn.elu,\\\n",
    "            kernel_regularizer=regularizer)\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_index, batch_size):\n",
    "    \n",
    "    #rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    #X_batch = X_train[rnd_indices]\n",
    "   # y_batch = y_train[rnd_indices]arr7[1:3,:]\n",
    "    X_batch = X_train[batch_index*batch_size:(batch_index+1)*batch_size,:]\n",
    "    y_batch = y_train[batch_index*batch_size:(batch_index+1)*batch_size,:]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = train_data_X_.shape[1]\n",
    "n_outputs = len(set(train_data_Y))\n",
    "learning_rate=0.01\n",
    "with tf.name_scope(\"input\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(lambd)\n",
    "fc2 = fc_layers(X,regularizer)\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc2, n_outputs, kernel_regularizer=regularizer,name=\"output3\")\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    loss_summary = tf.summary.scalar('loss', loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    acc_summary = tf.summary.scalar('acc', accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge([loss_summary, acc_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-4f1537951ee5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfinal_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./chickpoints/model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d%H%M%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./logs/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfile_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./chickpoints/model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./chickpoints/model\"\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = './logs/'+ now\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-2e10f1fb6233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_train_data_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         loss_val, summary_str,test_pred, test_acc = sess.run(\n",
      "\u001b[1;32m<ipython-input-71-b2a459901288>\u001b[0m in \u001b[0;36mrandom_batch\u001b[1;34m(X_train, y_train, batch_index, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#X_batch = X_train[rnd_indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m    \u001b[1;31m# y_batch = y_train[rnd_indices]arr7[1:3,:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(train_data_X.shape[0] / batch_size))\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(normal_train_data_X, train_data_Y,batch_index, batch_size)\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str,test_pred, test_acc = sess.run(\n",
    "                                        [loss, summary_op,predictions, accuracy],\\\n",
    "                                        feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val,\"\\tAcc:\",test_acc)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_pred = predictions.eval(feed_dict={X: X_test, y: y_test})\n",
    "    print('precision_score',precision_score(y_test, y_pred))\n",
    "    print('recall_score',recall_score(y_test, y_pred))\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "       [ 7,  8,  9, 10, 11, 12, 13],\n",
       "       [14, 15, 16, 17, 18, 19, 20],\n",
       "       [21, 22, 23, 24, 25, 26, 27],\n",
       "       [28, 29, 30, 31, 32, 33, 34]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr7 = np.arange(35).reshape(5,7)#生成一个5*7的数组\n",
    "\n",
    "arr7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8,  9, 10, 11, 12, 13],\n",
       "       [14, 15, 16, 17, 18, 19, 20]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "arr7[1:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
