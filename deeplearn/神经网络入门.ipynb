{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "#### 1.Loss Function 损失函数\n",
    "    | y-yhat |,是定义在单个样本上的，算的是一个样本的误差。\n",
    "#### 2.Cost Function 代价函数\n",
    "     1/N*Σ|y-yhat| , 一般是针对总体，是所有样本误差的平均，也就是损失函数的平均。\n",
    "#### 3.Object Function 目标函数 \n",
    "    1/N*Σ|y-yhat| +正则化项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 平方损失函数（MSE）（线性回归）\n",
    "    平方损失函数 均方误差(MSE)是最常用的回归损失函数，计算方法是求预测值与真实值之间距离的平方和，\n",
    "    公式 MSE=1/N*Σ|y-yhat|**2\n",
    "\n",
    "##### 平均绝对误差（MAE）\n",
    "    平均绝对误差（MAE）是另一种用于回归模型的损失函数。MAE是目标值和预测值之差的绝对值之和。其只衡量了预测值误差的平均模长，而不考虑方向\n",
    "    公式 MAE=1/N*Σ|y-yhat|\n",
    "\n",
    "    简单来说，MSE计算简便，但MAE对异常点有更好的鲁棒性。\n",
    "    如果异常点代表在商业中很重要的异常情况，并且需要被检测出来，则应选用MSE损失函数。相反，如果只把异常值当作受损数据，则应选用MAE损失函数。\n",
    "\n",
    "##### 对数损失函数（逻辑回归）\n",
    "    取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。\n",
    "\n",
    "     L=-1/N Σ y*log(yhat)+(1-y)*log(1-yhat)\n",
    "\n",
    "##### 指数损失函数（Adaboost）\n",
    "    学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，\n",
    "\n",
    "     L=1/N Σ exp[-y*yhat]\n",
    "\n",
    "\n",
    "##### Hinge损失函数（SVM）\n",
    "    在机器学习算法中，hinge损失函数和SVM是息息相关的。\n",
    "\n",
    "\n",
    "##### 决策树：\n",
    "    0-1损失函数：  L=1 (y!=yhat)   L=0 (y=yhat)\n",
    "\n",
    "    绝对值损失函数：L= |y-yhat|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导数\n",
    "    反映的是函数y=f(x)在某一点处沿x轴正方向的变化率。再强调一遍， 是函数f(x)在x轴上某一点处沿着x轴正方向的变化率/变化趋势。直观地看，也就是在x轴上某一点处，如果f’(x)>0，说明f(x)的函数值在x点沿x轴正方向是趋于增加的；如果f’(x)<0，说明f(x)的函数值在x点沿x轴正方向是趋于减少的。\n",
    "\n",
    "#### 偏导数\n",
    "    导数与偏导数本质是一致的，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。 \n",
    "     区别在于： \n",
    "     导数，指的是一元函数中，函数y=f(x)在某一点处沿x轴正方向的变化率； \n",
    "     偏导数，指的是多元函数中，函数y=f(x1,x2,…,xn)在某一点处沿某一坐标轴（x1,x2,…,xn）正方向的变化率。\n",
    "\n",
    "#### 梯度定义如下： \n",
    "     函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 \n",
    "     这里注意三点： \n",
    "     1）梯度是一个向量，即有方向有大小； \n",
    "     2）梯度的方向是最大方向导数的方向； \n",
    "     3）梯度的值是最大方向导数的值。 \n",
    "\n",
    "\n",
    "#### 梯度下降法\n",
    "    用梯度下降法（Gradient Descent）算法来最小化 L(θ)  Cost function，以计算出合适的所有参数 θ(w和b的值)\n",
    "    既然在变量空间的某一点处，函数沿梯度方向具有最大的变化率，那么在优化目标函数的时候，自然是沿着负梯度方向去减小函数值，以此达到我们的优化目标。 \n",
    "\n",
    "#### 步长(learning rate)(学习速度)\n",
    "    步长决定了在梯度下降过程中，每一步沿梯度负方向前进的长度。\n",
    "\n",
    "#### 梯度下降法描述：\n",
    "    计算损失函数对所有参数的（负）梯度  （dJ(θ)/dθ）\n",
    "    按梯度的负方向下降一定步长（直接加上负梯度） （θ = θ -α*dJ(θ)/dθ）\n",
    "    重复以上步骤，直到满足精度要求\n",
    "\n",
    "\n",
    "#### 反向传播\n",
    "    反向传播算法是求解函数梯度的一种方法，其本质上是利用链式法则对每个参数求偏导\n",
    "    反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数\n",
    "\n",
    "    反向传播算法计算的是单个训练样本对所有权重和偏置的调整——包括每个参数的正负变化以及变化的比例——使其能最快的降低损失。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过一个逻辑回归的实例来理解上面的概念\n",
    "\n",
    "    -初始化参数(w,b) \n",
    "    - 优化 loss function 学习得到参数 (w,b): \n",
    "    - 计算 cost 和gradient \n",
    "    - 通过梯度下降修改参数w,b\n",
    "    - 用学到的 (w,b) 去预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.定义所需函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    s=1.0/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.梯度下降获取参数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w,b,X,Y,num_iterations,learning_rate,print_cost):\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        #FORWARD PROPAGATION\n",
    "        A = sigmoid(np.dot(w.T, X)+b) # compute activation\n",
    "        cost = -(1.0/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute cost\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after interation %i:%f\"%(i,cost))\n",
    "\n",
    "        # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "        dw = (1.0/m)*np.dot(X,(A-Y).T)\n",
    "        db = (1.0/m)*np.sum(A-Y)\n",
    "\n",
    "        # update rule\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0,i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "            \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test ,num_iterations,learning_rate,print_cost):\n",
    "       \n",
    "    dim = X_train.shape[0]\n",
    "    w,b = initialize_with_zeros(dim)\n",
    "    parameters,grads,costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)\n",
    "    w = parameters['w']\n",
    "    b = parameters['b']\n",
    " \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    " \n",
    "    print('train accuracy:{}%'.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print('test accuracy:{}%'.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    d = {'costs' : costs,\n",
    "         'Y_prediction_test' : Y_prediction_test,\n",
    "         'Y_prediction_train' : Y_prediction_train,\n",
    "         'w' : w,\n",
    "         'b' : b,\n",
    "         'learning_rate' : learning_rate,\n",
    "         'num_iterations' : num_iterations\n",
    "         }\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络包含输入层，输出层，中间层（也叫隐藏层）。设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第l层神经网络，单个样本其各个参数的矩阵维度为：\n",
    "\n",
    "    W[l]:(n[l],n[l-1])  \n",
    "    b[l]:(n[l],1)  \n",
    "    dW[l]:(n[l],n[l-1])  \n",
    "    db[l]:(n[l],1)  \n",
    "    Z[l]:(n[l],1)  \n",
    "    A[l]:(n[l],1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数\n",
    "激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。\n",
    "如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。\n",
    "\n",
    "\n",
    "#### sigmoid函数\n",
    "优点：\n",
    "\n",
    "    1.Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。\n",
    "    2.求导容易。\n",
    "缺点：\n",
    "\n",
    "    1.由于其软饱和性，容易产生梯度消失，导致训练出现问题。\n",
    "    2.其输出并不是以0为中心的。\n",
    "\n",
    "#### tanh函数\n",
    "优点：\n",
    "\n",
    "    1.比Sigmoid函数收敛速度更快。\n",
    "    2.相比Sigmoid函数，其输出以0为中心。\n",
    "缺点：\n",
    "\n",
    "    还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。\n",
    "\n",
    "#### ReLU函数\n",
    "ReLU是最近几年非常受欢迎的激活函数。\n",
    "但是除了ReLU本身的之外，TensorFlow还提供了一些相关的函数，比如定义为min(max(features, 0), 6)的tf.nn.relu6(features, name=None)；或是CReLU，即tf.nn.crelu(features, name=None)。其中(CReLU部分可以参考这篇论文)。\n",
    "\n",
    "优点：\n",
    "\n",
    "    1.相比起Sigmoid和tanh，ReLU(e.g. a factor of 6 in Krizhevsky et al.)在SGD中能够快速收敛。例如在下图的实验中，在一个四层的卷积神经网络中，实线代表了ReLU，虚线代表了tanh，ReLU比起tanh更快地到达了错误率0.25处。据称，这是因为它线性、非饱和的形式。\n",
    "    2.Sigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。\n",
    "    3.有效缓解了梯度消失的问题。\n",
    "    4.在没有无监督预训练的时候也能有较好的表现。\n",
    "    5.提供了神经网络的稀疏表达能力。\n",
    "缺点：\n",
    "\n",
    "    随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了\n",
    "    \n",
    "#### -LReLU\n",
    "当aiai比较小而且固定的时候，我们称之为LReLU。LReLU最初的目的是为了避免梯度消失。但在一些实验中，我们发现LReLU对准确率并没有太大的影响。很多时候，当我们想要应用LReLU时，我们必须要非常小心谨慎地重复训练，选取出合适的aa，LReLU的表现出的结果才比ReLU好。因此有人提出了一种自适应地从数据中学习参数的PReLU。\n",
    "#### -PReLU\n",
    "PReLU是LReLU的改进，可以自适应地从数据中学习参数。PReLU具有收敛速度快、错误率低的特点。PReLU可以用于反向传播的训练，可以与其他层同时优化。\n",
    "\n",
    "#### 激活函数的选择：\n",
    "sigmoid函数和tanh函数比较：\n",
    "隐藏层：tanh函数的表现要好于sigmoid函数，因为tanh取值范围为[−1,+1]，输出分布在0值的附近，均值为0，从隐藏层到输出层数据起到了归一化（均值为0）的效果。\n",
    "输出层：对于二分类任务的输出取值为{0,1}，故一般会选择sigmoid函数。\n",
    "然而sigmoid和tanh函数在当|z|很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使|z|尽可能的落在0值附近。\n",
    "ReLU弥补了前两者的缺陷，当z>0时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。然而当z<0时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。\n",
    "Leaky ReLU保证在z<0的时候，梯度仍然不为0。\n",
    "在选择激活函数的时候，如果在不知道该选什么的时候就选择ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机初始化\n",
    "神经网络模型中的参数权重W是不能全部初始化为零的。\n",
    "\n",
    "如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。\n",
    "在初始化的时候，W参数要进行随机初始化，b则不存在对称性的问题它可以设置为0。\n",
    "\n",
    "以2个输入，2个隐藏神经元为例：\n",
    "\n",
    "    W = np.random.rand((2,2))* 0.01\n",
    "    b = np.zero((2,1))\n",
    "这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则Z=WX+bZ=WX+b所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。\n",
    "\n",
    "ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数与超参数\n",
    "参数：\n",
    "参数即是我们在过程中想要模型学习到的信息(W[l]，b[l])， \n",
    "超参数：\n",
    "超参数即为控制参数的输出值的一些网络信息，也就是超参数的改变会导致最终得到的参数W[l]，b[l]的改变。\n",
    "举例：\n",
    "学习速率：α\n",
    "迭代次数：N\n",
    "隐藏层的层数：L\n",
    "每一层的神经元个数：n[1]，n[2],⋯\n",
    "激活函数g(z)的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络算法\n",
    "1.为一个L-layer 的神经网络初始化参数\n",
    "\n",
    "2.实现前向传播模型\n",
    "\n",
    "    o定义线性函数 Z[l]=W[l]A[l-1]+b[l],A[0]=X\n",
    "    o选择激活函数(relu/sigmoid). A[l]=g(Z[l])\n",
    "    o结合以上两步定义为 [LINEAR->ACTIVATION] .\n",
    "    o循环执行[LINEAR->RELU]L-1 次( 1 到L-1层) ，在最后一层执行 [LINEAR->SIGMOID] \n",
    "3.计算损失函数. \n",
    "\n",
    "    cost = -np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m\n",
    "4.实现反向传播模型\n",
    "\n",
    "    o对于第L层我们需要\n",
    "    dW[l] = 1/m*dZ[l]A[l-1].T    \n",
    "    db[l] = 1/m*ΣdZ[l][i]   \n",
    "    dA[l-1] = W[l].T*dZ[l]\n",
    "    o定义激活函数的梯度下降函数(relu_backward/sigmoid_backward) dZ= relu_backward(dA)\n",
    "    o综合以上两个步骤 [LINEAR->ACTIVATION] 反向传播方法得到 dW ,db,dA\n",
    "    o循环执行[LINEAR->RELU] L-1  次( 1 到L-1层) 在最后一层执行 [LINEAR->SIGMOID]\n",
    "5.修改参数\n",
    "\n",
    "    W[l]=W[l] -αdW[l]\n",
    "    b[l]=b[l] -αdb[l]\n",
    "6.得到最终的W b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.定义所需函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    " \n",
    "    return A, cache\n",
    " \n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    " \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    " \n",
    "    assert (dZ.shape == Z.shape)\n",
    " \n",
    "    return dZ\n",
    " \n",
    "def relu(Z):\n",
    " \n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    " \n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    " \n",
    "    assert (dZ.shape == Z.shape)\n",
    " \n",
    "    return dZ\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "\n",
    "    np.random.seed(3)  #用于指定随机数生成时所用算法开始的整数值\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # 网络的层数\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.参数调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3反向传播\n",
    "    o对于第L层我们需要\n",
    "\n",
    "        dW[l] = 1/m*dZ[l]A[l-1].T    \n",
    "        db[l] = 1/m*ΣdZ[l][i]   \n",
    "        dA[l-1] = W[l].T*dZ[l]\n",
    "    o定义激活函数的梯度下降函数(relu_backward/sigmoid_backward) dZ= relu_backward(dA)\n",
    "    o综合以上两个步骤 [LINEAR->ACTIVATION] 反向传播方法得到 dW ,db,dA\n",
    "    o循环执行[LINEAR->RELU] L-1  次( 1 到L-1层) 在最后一层执行 [LINEAR->SIGMOID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 修改参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(parameters,X,Y,num_iterations,learning_rate,print_cost):\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        #FORWARD PROPAGATION\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after interation %i:%f\"%(i,cost))\n",
    "\n",
    "        # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # update rule\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_model(X,Y,layers_dims,learning_rate=0.0075,num_iterations=3000,print_cost=False,isPlot=True):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    (n_x,n_h,n_y) = layers_dims\n",
    "\n",
    "    \"\"\"\n",
    "    初始化参数\n",
    "    \"\"\"\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    \"\"\"\n",
    "    开始进行迭代\n",
    "    \"\"\"\n",
    "    parameters,costs= optimize(parameters,X,Y,num_iterations,learning_rate,print_cost)\n",
    "        \n",
    "        \n",
    "    #迭代完成，根据条件绘制图\n",
    "    if isPlot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    #返回parameters\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.测试\n",
    "假设我们的是一个两层模型，输入参数是，输出参数是，隐藏层一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 10\n",
    "n_h = 7\n",
    "n_y = 4\n",
    "layers_dims = (n_x,n_h,n_y)\n",
    "\n",
    "parameters = n_layer_model(train_x, train_set_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True,isPlot=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
