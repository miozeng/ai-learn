{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "testdata = pd.read_csv('../data/dataannalysis/happiness_test_abbr.csv')\n",
    "traindata = pd.read_csv('../data/dataannalysis/happiness_train_abbr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#异常数据处理\n",
    "traindata = traindata[traindata['happiness'] != -8]\n",
    "traindata = traindata[traindata['depression'] != -8]\n",
    "traindata = traindata[traindata['class'] != -8]\n",
    "traindata = traindata[traindata['health'] != -8]\n",
    "traindata = traindata[traindata['equity'] != -8]\n",
    "traindata = traindata[traindata['family_status'] != -8]\n",
    "traindata = traindata[traindata['health_problem'] != -8]\n",
    "traindata = traindata[traindata['relax'] != -8]\n",
    "traindata = traindata[traindata['learn'] != -8]\n",
    "traindata['view'].replace(-8,3,inplace = True)\n",
    "traindata = traindata[traindata['socialize'] != -8]\n",
    "traindata = traindata[traindata['socialize'] != -1]\n",
    "traindata = traindata[traindata['socialize'] != -2]\n",
    "traindata = traindata[traindata['socialize'] != -3]\n",
    "traindata = traindata[traindata['socialize'] != 50]\n",
    "traindata = traindata[traindata['status_3_before'] != -8]\n",
    "traindata = traindata[traindata['status_peer'] != -8]\n",
    "traindata['inc_ability'].replace(-8,3,inplace = True)\n",
    "\n",
    "#数据重新处理\n",
    "traindata['marital'].replace(2,1,inplace = True)\n",
    "traindata['marital'].replace(3,1,inplace = True)\n",
    "traindata['marital'].replace(4,1,inplace = True)\n",
    "traindata['marital'].replace(7,1,inplace = True)\n",
    "traindata['marital'].replace(6,2,inplace = True)\n",
    "traindata['marital'].replace(5,3,inplace = True)\n",
    "\n",
    "bins = [-100,0,100000,300000,500000,1000000,10000000] \n",
    "traindata['income'] = pd.cut(traindata['income'],bins, labels=['1','2', '3', '4', '5', '6'])\n",
    "\n",
    "\n",
    "\n",
    "avg_income=traindata['family_income']/traindata['family_m']\n",
    "traindata['avg_income'] = avg_income\n",
    "bins = [-100,0,20000,50000,100000,1000000,10000000] \n",
    "traindata['avg_income'] = pd.cut(traindata['avg_income'],bins, labels=['1','2', '3', '4', '5', '6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindata['avg_income'].('unknown', inplace=True)\n",
    "traindata['avg_income'] = traindata['avg_income'].cat.add_categories(['0']);\n",
    "traindata['avg_income'].fillna('0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "le.fit(['1','2', '3', '4', '5', '6', '0']) \n",
    "traindata['avg_income']  = le.transform(traindata['avg_income'] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "le.fit(['1','2', '3', '4', '5', '6']) \n",
    "traindata['income']  = le.transform(traindata['income'] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0.008267\n",
       "happiness          1.000000\n",
       "survey_type       -0.035656\n",
       "province          -0.021285\n",
       "city              -0.023442\n",
       "county            -0.021499\n",
       "gender             0.018517\n",
       "birth             -0.004821\n",
       "nationality       -0.035595\n",
       "religion           0.006780\n",
       "religion_freq      0.010455\n",
       "edu                0.104231\n",
       "income             0.043733\n",
       "political          0.080010\n",
       "floor_area         0.044722\n",
       "height_cm          0.040638\n",
       "weight_jin         0.084260\n",
       "health             0.246918\n",
       "health_problem     0.212923\n",
       "depression         0.320490\n",
       "hukou              0.079546\n",
       "socialize          0.079689\n",
       "relax              0.123843\n",
       "learn              0.123454\n",
       "equity             0.284537\n",
       "class              0.301481\n",
       "work_exper         0.008198\n",
       "work_status       -0.039226\n",
       "work_yr           -0.018063\n",
       "work_type          0.021679\n",
       "work_manage       -0.039622\n",
       "family_income      0.052113\n",
       "family_m           0.059978\n",
       "family_status      0.306640\n",
       "house              0.092656\n",
       "car               -0.110458\n",
       "marital           -0.077905\n",
       "status_peer       -0.273150\n",
       "status_3_before   -0.177749\n",
       "view               0.133684\n",
       "inc_ability       -0.191048\n",
       "avg_income         0.102122\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重新做相关性分析\n",
    "traindata.corr()['happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindatafinal = traindata[['happiness','depression','depression','class','health','equity','family_status','health_problem','relax','learn','view','socialize','status_peer','status_3_before','inc_ability','car','avg_income','edu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = traindatafinal[:7000]\n",
    "test_data = traindatafinal[7000:]\n",
    " \n",
    "train_data_X = train_data.drop(['happiness'],axis=1)\n",
    "train_data_Y = train_data['happiness']\n",
    "test_data_X = test_data.drop(['happiness'],axis=1)\n",
    "test_data_Y = test_data['happiness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.定义所需函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    " \n",
    "    return A, cache\n",
    " \n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    " \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    " \n",
    "    assert (dZ.shape == Z.shape)\n",
    " \n",
    "    return dZ\n",
    " \n",
    "def relu(Z):\n",
    " \n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    " \n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    " \n",
    "    assert (dZ.shape == Z.shape)\n",
    " \n",
    "    return dZ\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "\n",
    "    np.random.seed(3)  #用于指定随机数生成时所用算法开始的整数值\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # 网络的层数\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.参数调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3反向传播\n",
    "    o对于第L层我们需要\n",
    "\n",
    "        dW[l] = 1/m*dZ[l]A[l-1].T    \n",
    "        db[l] = 1/m*ΣdZ[l][i]   \n",
    "        dA[l-1] = W[l].T*dZ[l]\n",
    "    o定义激活函数的梯度下降函数(relu_backward/sigmoid_backward) dZ= relu_backward(dA)\n",
    "    o综合以上两个步骤 [LINEAR->ACTIVATION] 反向传播方法得到 dW ,db,dA\n",
    "    o循环执行[LINEAR->RELU] L-1  次( 1 到L-1层) 在最后一层执行 [LINEAR->SIGMOID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 修改参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(parameters,X,Y,num_iterations,learning_rate,print_cost):\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        #FORWARD PROPAGATION\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after interation %i:%f\"%(i,cost))\n",
    "\n",
    "        # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # update rule\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_model(X,Y,layers_dims,learning_rate=0.0075,num_iterations=3000,print_cost=False,isPlot=True):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    (n_x,n_h,n_y) = layers_dims\n",
    "\n",
    "    \"\"\"\n",
    "    初始化参数\n",
    "    \"\"\"\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    \"\"\"\n",
    "    开始进行迭代\n",
    "    \"\"\"\n",
    "    parameters,costs= optimize(parameters,X,Y,num_iterations,learning_rate,print_cost)\n",
    "        \n",
    "        \n",
    "    #迭代完成，根据条件绘制图\n",
    "    if isPlot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    #返回parameters\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.测试\n",
    "假设我们的是一个两层模型，输入参数是，输出参数是，隐藏层一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (7,10) and (7000,17) not aligned: 10 (dim 1) != 7000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1e1176da5d45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0misPlot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-c980bf508f63>\u001b[0m in \u001b[0;36mn_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost, isPlot)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0m开始进行迭代\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-58d4a1529be8>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(parameters, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m#FORWARD PROPAGATION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-a41c2616bfa1>\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m### START CODE HERE ### (≈ 2 lines of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-9c9bae0b22d6>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlinear_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (7,10) and (7000,17) not aligned: 10 (dim 1) != 7000 (dim 0)"
     ]
    }
   ],
   "source": [
    "n_x = 17\n",
    "n_h = 7\n",
    "n_y = 4\n",
    "layers_dims = (n_x,n_h,n_y)\n",
    "\n",
    "parameters = n_layer_model(train_data_X, train_data_Y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True,isPlot=True)\n",
    "\n",
    "\n",
    "#test_data_X test_data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[l]:(n[l],n[l-1])  \n",
    "b[l]:(n[l],1)  \n",
    "dW[l]:(n[l],n[l-1])  \n",
    "db[l]:(n[l],1)  \n",
    "Z[l]:(n[l],1)  \n",
    "A[l]:(n[l],1)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
