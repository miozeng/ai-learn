{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络基础\n",
    "卷积神经网络的主要组成\n",
    "conv： \n",
    "\n",
    "    1.相当于一个特征提取器来提取特征\n",
    "    2.提供了位置信息\n",
    "    3.减少了参数个数\n",
    "pooling： \n",
    "\n",
    "    1.提取特征\n",
    "    2.减少参数\n",
    "\n",
    "激活函数：增加网络的非线性表达能力\n",
    "\n",
    "### 计算机视觉\n",
    "计算机视觉（Computer Vision）包含很多不同类别的问题，如图片分类、目标检测、图片风格迁移等等。\n",
    "对于小尺寸的图片问题，也许我们用深度神经网络的结构可以较为简单的解决一定的问题。但是当应用在大尺寸的图片上，输入规模将变得十分庞大，使用神经网络将会有非常多的参数需要去学习，这个时候神经网络就不再适用。\n",
    "卷积神经网络在计算机视觉问题上是一个非常好的网络结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 边缘检测示例\n",
    "卷积运算是卷积神经网络的基本组成部分。下面以边缘检测的例子来介绍卷积运算。\n",
    "垂直边缘检测：\n",
    "假设对于一个 6×6 大小的图片（以数字表示），以及一个 3×3大小的 filter（卷积核） 进行卷积运算，以“∗”符号表示。图片和垂直边缘检测器分别如左和中矩阵所示：\n",
    "\n",
    "![title](img/filter.png)\n",
    "filter 不断地和其大小相同的部分做对应元素的乘法运算并求和，最终得到的数字相当于新图片的一个像素值，如右矩阵所示，最终得到一个 4×4小的图片。\n",
    "\n",
    "边缘检测的原理：\n",
    "以一个有一条垂直边缘线的简单图片来说明。通过垂直边缘 filter 我们得到的最终结果图片可以明显地将边缘和非边缘区分出来：\n",
    " ![title](img/filter2.png)\n",
    "\n",
    "卷积运算提供了一个方便的方法来检测图像中的边缘，成为卷积神经网络中重要的一部分。\n",
    "\n",
    "多种边缘检测：\n",
    "垂直和水平边缘检测\n",
    "\n",
    "![title](img/vhfilter.png)\n",
    "更复杂的filter包括sobel filter scharr filter等。可以参考计算机视觉里面的说明\n",
    "\n",
    "对于复杂的图片，我们可以直接将filter中的数字直接看作是需要学习的参数，其可以学习到对于图片检测相比上面filter更好的更复杂的filter，如相对于水平和垂直检测器，我们训练的 filter 参数也许可以知道不同角度的边缘。\n",
    "\n",
    "通过卷积运算，在卷积神经网络中通过反向传播算法，可以学习到相应于目标结果的filter，将其应用于整个图片，输出其提取到的所有有用的特征。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "没有Padding的缺点：\n",
    "\n",
    "    每次卷积操作，图片会缩小；\n",
    "    \n",
    "        就前面的例子来说，6×6大小的图片，经过 3×3 大小的 filter，缩小成了 4×4大小 \n",
    "        图片：n×n –> (n−f+1)×(n−f+1)\n",
    "        \n",
    "    角落和边缘位置的像素进行卷积运算的次数少，可能会丢失有用信息。\n",
    "\n",
    "其中，n表示图片的长或宽的大小，f表示filter的长或宽的大小。\n",
    "\n",
    "加Padding：\n",
    "\n",
    "为了解决上面的两个缺点，我们在进行卷积运算前为图片加padding，包围角落和边缘的像素，使得通过filter的卷积运算后，图片大小不变，也不会丢失角落和边沿的信息。\n",
    "![title](img/padding.png)\n",
    "\n",
    "以p表示 Padding 的值，则输入n×n大小的图片，最终得到的图片大小为 (n+2p−f+1)×(n+2p−f+1)，为使图片大小保持不变，需根据filter的大小调整p的值。\n",
    "\n",
    "Valid / Same 卷积：\n",
    "\n",
    "- Valid：no padding；（n×n–> (n−f+1)×(n−f+1)\n",
    "- Same：padding，输出与输入图片大小相同，（p=(f−1)/2）。在计算机视觉中，一般来说padding的值为奇数（因为filter一般为奇数）p-->padding\n",
    "\n",
    "我们通常让f是一个奇数。一个原因是奇数可以更好的使用same，还有一个原因是奇数的矩阵会有一个中心点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积步长（stride）\n",
    "卷积的步长是构建卷积神经网络的一个基本的操作。卷积核每次走几个格子\n",
    "以s表示stride的大小，那么在进行卷积运算后，图片的变化为：n×n –>\n",
    "![title](img/stride.png)\n",
    "注意，在当padding≠1时，若移动的窗口落在图片外面时，则不要再进行相乘的操作，丢弃边缘的数值信息，所以输出图片的最终维度为向下取整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  立体卷积\n",
    "卷积核的通道数：\n",
    "\n",
    "对于灰色图像中，卷积核和图像均是二维的。而应用于彩色图像中，因为图片有R、G、B三个颜色通道，所以此时的卷积核应为三维卷积核。\n",
    "![title](img/MC.png)\n",
    "\n",
    "卷积核的第三个维度需要与进行卷积运算的图片的通道数相同。\n",
    "\n",
    "多卷积核：\n",
    "单个卷积核应用于图片时，提取图片特定的特征，不同的卷积核提取不同的特征。如两个大小均为3×3×3的卷积核分别提取图片的垂直边缘和水平边缘。\n",
    "\n",
    "![title](img/mfilter.png)\n",
    "由图可知，最终提取到彩色图片的垂直特征图和水平特征图，得到有2个通道的4×4大小的特征图片。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单卷积网络\n",
    "单层卷积网络的例子：\n",
    "\n",
    "![title](img/singlecov.png)\n",
    "和普通的神经网络单层前向传播的过程类似，卷积神经网络也是一个先由输入和权重及偏置做线性运算，然后得到的结果输入一个激活函数中，得到最终的输出\n",
    "\n",
    "![title](img/cov.png)\n",
    "![title](img/cov2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层\n",
    "\n",
    "池化包含最大池化（Max pooling）：\n",
    "最大池化是对前一层得到的特征图进行池化减小，仅由当前小区域内的最大值来代表最终池化后的值。\n",
    "\n",
    "平均池化（Average pooling）：\n",
    "平均池化与最大池化唯一不同的是其选取的是小区域内的均值来代表该区域内的值。\n",
    "\n",
    "在最大池化中，有一组超参数需要进行调整，其中，f表示池化的大小，s表示步长。\n",
    "池化前：n池化后 ((n+2p-f)/s)+1\n",
    "\n",
    "\n",
    "池化 Summary：\n",
    "\n",
    "池化层的超参数：\n",
    "\n",
    "    f：filter的大小；\n",
    "    s：stride大小；\n",
    "    最大池化或者平均池化；\n",
    "    p：padding，这里要注意，几乎很少使用。\n",
    "注意，池化层没有需要学习的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络示例\n",
    "这里以 LeNet-5 为例，给出一个完整的卷积神经网络。\n",
    "\n",
    "构建深度卷积的模式：\n",
    "\n",
    "    随着网络的深入，提取的特征图片大小将会逐渐减小，但同时通道数量应随之增加；\n",
    "    Conv——Pool——Conv——Pool——Fc——Fc——Fc——softmax。\n",
    "\n",
    "训练卷积神经网络：\n",
    "我们将训练集输入到卷积神经网络中，对网络进行训练。利用梯度下降（Adam、momentum等优化算法）最小化代价函数来寻找网络最优的参数。\n",
    "\n",
    "### 卷积神经网络优点\n",
    "- 参数少的优势：\n",
    "与普通的全连接神经网络相比，卷积神经网络的参数更少。如例子将32×32×3 转换为一个28×28×6的结果，卷积神经网络仅有6×(5×5+1)=156个参数，而普通的全连接网络有3072×4704≈14M个参数。\n",
    "\n",
    "- 参数共享：一个特征检测器（filter）对图片的一部分有用的同时也有可能对图片的另外一部分有用。\n",
    "- 连接的稀疏性：在每一层中，每个输出值只取决于少量的输入。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度卷积模型\n",
    "###  经典的卷积网络\n",
    "介绍几种经典的卷积神经网络结构，分别是LeNet、AlexNet、VGGNet。\n",
    "\n",
    "#### LeNet-5： \n",
    "LeNet-5主要是针对灰度设计的，所以其输入较小，为32×32×1，其结构如下：\n",
    "![title](img/lenet-5.png)\n",
    "在LetNet中，存在的经典模式：\n",
    "随着网络的深度增加，图像的大小在缩小，与此同时，通道的数量却在增加；每个卷积层后面接一个池化层。\n",
    "\n",
    "\n",
    "#### AlexNet：\n",
    "https://www.jianshu.com/p/00a53eb5f4b3\n",
    "AlexNet直接对彩色的大图片进行处理，其结构如下：\n",
    "![title](img/alexnet.png)\n",
    "\n",
    "网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布网络包含8个带权重的层；\n",
    "\n",
    "卷积层C1\n",
    "input（224X224X3）-->数据增强（227X227X3)-->卷积(11X11X3的卷积核，步长为4，进行特征提取（55X55X96）)-->relu\n",
    "-->池化层（降采样操作，核：3X3 步长：2，(27X27X96)）\n",
    "\n",
    "    [注意：Alexnet中采用的是最大池化，是为了避免平均池化的模糊化效果，从而保留最显著的特征,并且AlexNet中提出让步长比池化核的尺寸小,这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性，减少了信息的丢失。]\n",
    "\n",
    "卷积层的参数 = 卷积核的数量 * 卷积核 + 偏置\n",
    "C1: 96个11×11×3的卷积核，96×11×11×3+96=34848\n",
    "\n",
    "卷积层C2\n",
    "input（27X27X96）-->卷积(5X5X256 的卷积核，步长为1，进行特征提取（55X55X96）)-->\n",
    "relu-->池化层（降采样操作，核：3X3 步长：2，(13X13X96 )）\n",
    "1.输入数据 27X27X96\n",
    "2.conv2中使用256个5X5大小的过滤器filter(步长1)对27X27X96个特征图，进行进一步提取特征，但是处理的方式和conv1不同，过滤器是对96个特征图中的某几个特征图中相应的区域乘以相应的权重，然后加上偏置之后所得到区域进行卷积。经过这样卷积之后，然后在在加上宽度高度两边都填充2像素，会的到一个新的256个特征图.特征图的大小为：\n",
    "(【27+2X2 - 5】/1 +1） = 27 ，也就是会有256个27X27大小的特征图.\n",
    "3.然后进行relu操作,relu之后的数据27X27X96\n",
    "4.降采样操作pool2\n",
    "pool1的核：3X3 步长：2，pool2（池化层）降采样之后的数据为13X13X96 [(27-3)/2+1=13]\n",
    "\n",
    "\n",
    "卷积层C3\n",
    "1.没有降采样层\n",
    "2.得到【13+2X1 -3】/1 +1 = 13 ， 384个13X13的新特征图（核3X3,步长为1）\n",
    "该层的处理流程是： 卷积-->ReLU\n",
    "卷积，输入是13×13×256，使用2组共384尺寸为3×3×256的卷积核，做了边缘填充padding=1，卷积的步长为1.则输出的FeatureMap为13×13 times384\n",
    "ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n",
    "卷积层C4\n",
    "1.没有降采样层\n",
    "2.得到【13+2X1 -3】/1 +1 = 13 ， 384个13X13的新特征图（核3X3,步长为1）\n",
    "该层的处理流程是： 卷积-->ReLU\n",
    "该层和C3类似。\n",
    "卷积，输入是13×13×384，分为两组，每组为13×13×192.使用2组，每组192个尺寸为3×3×192的卷积核，做了边缘填充padding=1，卷积的步长为1.则输出的FeatureMap为13×13 times384，分为两组，每组为13×13×192\n",
    "ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n",
    "卷积层C5\n",
    "1.输出数据为13X13X256的特征图\n",
    "2.降采样操作pool3**\n",
    "pool3的核：3X3 步长：2，pool3（池化层）降采样之后的数据为6X6X256 [(13-3)/2+1=6]\n",
    "该层处理流程为：卷积-->ReLU-->池化\n",
    "卷积，输入为13×13×384，分为两组，每组为13×13×192。使用2组，每组为128尺寸为3×3×192的卷积核，做了边缘填充padding=1，卷积的步长为1.则输出的FeatureMap为13×13×256\n",
    "ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n",
    "池化，池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为 (13−3)/2+1=6,即池化后的输出为6×6×256\n",
    "全连接层FC6\n",
    "\n",
    "全连接层，这里使用4096个神经元，对256个大小为6X6特征图，进行一个全连接，也就是将6X6大小的特征图，进行卷积变为一个特征点,然后对于4096个神经元中的一个点，是由256个特征图中某些个特征图卷积之后得到的特征点乘以相应的权重之后，再加上一个偏置得到,之后再进行一个dropout，也就是随机从4096个节点中丢掉一些节点信息（值清0），然后就得到新的4096个神经元。\n",
    "（dropout的使用可以减少过度拟合，丢弃并不影响正向和反向传播。）\n",
    "[注意：在经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。]\n",
    "\n",
    "\n",
    "该层的流程为：（卷积）全连接 -->ReLU -->Dropout\n",
    "卷积->全连接： 输入为6×6×256,该层有4096个卷积核，每个卷积核的大小为6×6×256。由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为4096×1×1，即有4096个神经元。\n",
    "ReLU,这4096个运算结果通过ReLU激活函数生成4096个值\n",
    "Dropout,抑制过拟合，随机的断开某些神经元的连接或者是不激活某些神经元\n",
    "全连接层FC7\n",
    "和fc6类似\n",
    "\n",
    "流程为：全连接-->ReLU-->Dropout\n",
    "全连接，输入为4096的向量\n",
    "ReLU,这4096个运算结果通过ReLU激活函数生成4096个值\n",
    "Dropout,抑制过拟合，随机的断开某些神经元的连接或者是不激活某些神经元\n",
    "输出层\n",
    "采用的是1000个神经元，然后对fc7中4096个神经元进行全链接，然后会通过高斯过滤器，得到1000个float型的值，也就是我们所看到的预测的可能性。\n",
    "第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，这就是预测结果。\n",
    "\n",
    "    \n",
    "    更深的网络结构\n",
    "    使用层叠的卷积层，即卷积层+卷积层+池化层来提取图像的特征\n",
    "    使用Dropout抑制过拟合\n",
    "    使用数据增强Data Augmentation抑制过拟合\n",
    "    使用Relu替换之前的sigmoid的作为激活函数\n",
    "    多GPU训练\n",
    "\n",
    "数据增强\n",
    "AlexNet中对数据做了以下操作：\n",
    "    随机裁剪，对256×256的图片进行随机裁剪到227×227，然后进行水平翻转。\n",
    "    测试的时候，对左上、右上、左下、右下、中间分别做了5次裁剪，然后翻转，共10个裁剪，之后对结果求平均。\n",
    "    对RGB空间做PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了1%。\n",
    "\n",
    "层叠的卷积层\n",
    "AlexNet池化的大小为3×3的正方形，每次池化移动步长为2，这样就会出现重叠。重叠池化可以避免过拟合\n",
    "\n",
    "AlexNet参数数量\n",
    "卷积层的参数 = 卷积核的数量 * 卷积核 + 偏置\n",
    "\n",
    "C1: 96个11×11×3的卷积核，96×11×11×3+96=34848\n",
    "C2: 2组，每组128个5×5×48的卷积核，(128×5×5×48+128)×2=307456\n",
    "C3: 384个3×3×256的卷积核，3×3×256×384+384=885120\n",
    "C4: 2组，每组192个3×3×192的卷积核，(3×3×192×192+192)×2=663936\n",
    "C5: 2组，每组128个3×3×192的卷积核，(3×3×192×128+128)×2=442624\n",
    "FC6: 4096个6×6×256的卷积核，6×6×256×4096+4096=37752832\n",
    "FC7: 4096∗4096+4096=16781312\n",
    "output: 4096∗1000=4096000\n",
    "卷积层 C2,C4,C5中的卷积核只和位于同一GPU的上一层的FeatureMap相连。从上面可以看出，参数大多数集中在全连接层，在卷积层由于权值共享，权值参数较少。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 创建模型序列\n",
    "model = Sequential()\n",
    "#第一层卷积网络，使用96个卷积核，大小为11x11步长为4， 要求输入的图片为227x227， 3个通道，不加边，激活函数使用relu\n",
    "model.add(Conv2D(96, (11, 11), strides=(1, 1), input_shape=(28, 28, 1), padding='same', activation='relu',\n",
    "                 kernel_initializer='uniform'))\n",
    "# 池化层\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "# 第二层加边使用256个5x5的卷积核，加边，激活函数为relu\n",
    "model.add(Conv2D(256, (5, 5), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
    "#使用池化层，步长为2\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "# 第三层卷积，大小为3x3的卷积核使用384个\n",
    "model.add(Conv2D(384, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
    "# 第四层卷积,同第三层\n",
    "model.add(Conv2D(384, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
    "# 第五层卷积使用的卷积核为256个，其他同上\n",
    "model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()\n",
    "参考来源：(https://blog.csdn.net/qq_41559533/article/details/83718778 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG-16：\n",
    "VGG卷积层和池化层均具有相同的卷积核大小，都使用3×3，stride=1,SAME的卷积的2×2，stride=2的池化。其结构如下：\n",
    "![title](img/vgg2.jpg)\n",
    "\n",
    "    1、输入224x224x3的图片，经个3x3x64的卷积核作两次卷积+ReLU，卷积后的尺寸变为224x224x64\n",
    "    2、作max pooling（最大化池化），池化单元尺寸为2x2（效果为图像尺寸减半），池化后的尺寸变为112x112x64\n",
    "    3、经128个3x3的卷积核作两次卷积+ReLU，尺寸变为112x112x128\n",
    "    4、作2x2的max pooling池化，尺寸变为56x56x128\n",
    "    5、经256个3x3的卷积核作三次卷积+ReLU，尺寸变为56x56x256\n",
    "    6、作2x2的max pooling池化，尺寸变为28x28x256\n",
    "    7、经512个3x3的卷积核作三次卷积+ReLU，尺寸变为28x28x512\n",
    "    8、作2x2的max pooling池化，尺寸变为14x14x512\n",
    "    9、经512个3x3的卷积核作三次卷积+ReLU，尺寸变为14x14x512\n",
    "    10、作2x2的max pooling池化，尺寸变为7x7x512\n",
    "    11、与两层1x1x4096，一层1x1x1000进行全连接+ReLU（共三层）\n",
    "    12、通过softmax输出1000个预测结果\n",
    "\n",
    "\n",
    "    INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\n",
    "    CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\n",
    "    CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\n",
    "    POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\n",
    "    CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\n",
    "    CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\n",
    "    POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\n",
    "    CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n",
    "    CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\n",
    "    CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\n",
    "    POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\n",
    "    CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\n",
    "    CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\n",
    "    CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\n",
    "    POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\n",
    "    CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\n",
    "    CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\n",
    "    CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\n",
    "    POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\n",
    "    FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\n",
    "    FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\n",
    "    FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n",
    "\n",
    "    TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)\n",
    "    TOTAL params: 138M parameters\n",
    "\n",
    "\n",
    "##### 特点：\n",
    "1、结构简洁\n",
    "VGG由5层卷积层、3层全连接层、softmax输出层构成，层与层之间使用max-pooling（最大化池）分开，所有隐层的激活单元都采用ReLU函数。\n",
    "\n",
    "2、小卷积核和多卷积子层\n",
    "VGG使用多个较小卷积核（3x3）的卷积层代替一个卷积核较大的卷积层，一方面可以减少参数，另一方面相当于进行了更多的非线性映射，可以增加网络的拟合/表达能力。\n",
    "小卷积核是VGG的一个重要特点，虽然VGG是在模仿AlexNet的网络结构，但没有采用AlexNet中比较大的卷积核尺寸（如7x7），而是通过降低卷积核的大小（3x3），增加卷积子层数来达到同样的性能（VGG：从1到4卷积子层，AlexNet：1子层）。3*3是最小的能够捕捉像素8邻域信息的的尺寸。\n",
    "2个3*3卷积堆叠等于1个5*5卷积，3个3*3堆叠等于1个7*7卷积，感受野大小不变，而采用更多层、更小的卷积核可以引入更多非线性（更多的隐藏层，从而带来更多非线性函数），提高决策函数判决力，并且带来更少参数。\n",
    "\n",
    "3、小池化核\n",
    "相比AlexNet的3x3的池化核，VGG全部采用2x2的池化核。\n",
    "\n",
    "4、通道数多\n",
    "VGG网络第一层的通道数为64，后面每层都进行了翻倍，最多到512个通道，通道数的增加，使得更多的信息可以被提取出来。\n",
    "\n",
    "5、层数更深、特征图更宽\n",
    "由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，控制了计算量的增加规模。\n",
    "\n",
    "6、全连接转卷积（测试阶段）\n",
    "这也是VGG的一个特点，在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入，这在测试阶段很重要。\n",
    "\n",
    "使用1*1的卷积层来增加线性变换，输出的通道数量上并没有发生改变。这里提一下1*1卷积层的其他用法，1*1的卷积层常被用来提炼特征，即多通道的特征组合在一起，凝练成较大通道或者较小通道的输出，而每张图片的大小不变。有时1*1的卷积神经网络还可以用来替代全连接层。\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "ResNet是由残差块所构建。\n",
    "残差块：\n",
    "一个普通的神经网络块的传输，其前向传播的计算步骤为：\n",
    "\n",
    "![title](img/forward.png)\n",
    "a[l+2]=g(z[l+2])\n",
    "而ResNet块则将其传播过程增加了一个从a[l]直接到z[l+2]的连接，将其称之为“short cut”或者“skip connection”：\n",
    "\n",
    "![title](img/rest.png)\n",
    "也就是前向传播公式的最后一个步骤变为：a[l+2]=g(z[l+2]+a[l])\n",
    "注意这里是连接在Relu激活函数之前。\n",
    "\n",
    "\n",
    "#### Residual Network：\n",
    "多个残差块堆积起来构成ResNet网络结构\n",
    "\n",
    "- 在没有残差的普通神经网络中，训练的误差实际上是随着网络层数的加深，先减小再增加；\n",
    "- 在有残差的ResNet中，即使网络再深，训练误差都会随着网络层数的加深逐渐减小。\n",
    "ResNet对于中间的激活函数来说，有助于能够达到更深的网络，解决梯度消失和梯度爆炸的问题。\n",
    "\n",
    "#### ResNet表现好的原因\n",
    "假设网络中均使用Relu激活函数，所以最后的输出a⩾0。这里我们给出a[l+2]的值：\n",
    "![title](img/oldrelu.png)\n",
    "如果使用L2正则化或者权重衰减，会压缩W和b的值，如果W[l+2]=0同时b[l+2]=0，那么上式就变成：\n",
    "![title](img/newrelu.png)\n",
    "\n",
    "所以从上面的结果我们可以看出，对于残差块来学习上面这个恒等函数是很容易的。所以在增加了残差块后更深的网络的性能也并不逊色于没有增加残差块简单的网络。所以尽管增加了网络的深度，但是并不会影响网络的性能。同时如果增加的网络结构能够学习到一些有用的信息，那么就会提升网络的性能。\n",
    "同时由于结构a[l+2]=g(z[l+2]+a[l])，ResNet在设计中使用了很多相同的卷积，以保持z[l+2]z[l+2]和a[l]a[l]的维度相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1x1卷积\n",
    "在三维上，与1×1×nC卷积核进行卷积，相当于三维图像上的1×1×nC的切片，也就是nC个点乘以卷积数值权重，通过Relu函数后，输出对应的结果。而不同的卷积核则相当于不同的隐层神经元结点与切片上的点进行一一连接。\n",
    "所以根本上1×1卷积核相当于对一个切片上的nC个单元都应用了一个全连接的神经网络。\n",
    "1x1卷积应用：\n",
    "\n",
    "    维度压缩：使用目标维度的1×1的卷积核个数。\n",
    "    增加非线性：保持与原维度相同的1×1的卷积核个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Network\n",
    "Inception Network 的作用就是使我们无需去考虑在构建深度卷积神经网络时，使用多大的卷积核以及是否添加池化层等问题。\n",
    "Inception主要结构：\n",
    "![title](img/Inception.png)\n",
    "\n",
    "在上面的Inception结构中，应用了不同的卷积核，以及带padding的池化层。在保持输入图片大小不变的情况下，通过不同运算结果的叠加，增加了通道的数量。\n",
    "\n",
    "#### Inception Network：\n",
    "多个Inception 模块的堆叠构成Inception Network，下面是GoogleNet的结构：\n",
    "![title](img/GoogleNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迁移学习\n",
    "#### 小数据集：\n",
    "如今在深度学习领域，许多研究者都会将他们的工作共享到网络上。在我们实施自己的工作的时候，比如说做某种物体的识别分类，但是只有少量的数据集，对于从头开始训练一个深度网络结构是远远不够的。\n",
    "但是我们可以应用迁移学习，应用其他研究者建立的模型和参数，用少量的数据仅训练最后自定义的softmax网络。从而能够在小数据集上达到很好的效果。\n",
    "\n",
    "#### 大数据集：\n",
    "如果我们在自己的问题上也拥有大量的数据集，我们可以多训练后面的几层。总之随着数据集的增加，我们需要“ freeze”的层数越来越少。最后如果我们有十分庞大的数据集，那么我们可以训练网络模型的所有参数，将其他研究者训练的模型参数作为参数的初始化来替代随机初始化，来加速我们模型的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 数据扩充\n",
    "与其他机器学习问题相比，在计算机视觉领域当下最主要的问题是没有办法得到充足的数据。所以在我们训练计算机数据模型的时候，数据的扩充就是会非常有用。\n",
    "数据扩充的方法：\n",
    "\n",
    "    镜像翻转（Mirroring）；\n",
    "    随机剪裁（Random Cropping）；\n",
    "    色彩转换（Color shifting）： \n",
    "    为图片的RGB三个色彩通道进行增减值，如（R：+20，G：-20，B：+20）；PCA颜色增强：对图片的主色的变化较大，图片的次色变化较小，使总体的颜色保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标检测\n",
    "\n",
    "#### 目标定位和特征点检测\n",
    "\n",
    "图片检测问题：\n",
    "\n",
    "    分类问题：判断图中是否为汽车；\n",
    "    目标定位：判断是否为汽车，并确定具体位置；\n",
    "    目标检测：检测不同物体并定位。\n",
    "\n",
    "##### 目标定位分类\n",
    "对应目标定位分类问题，其实是为了得到目标的矩形框，目标的坐标，长度和宽度，以及得到目标的类型\n",
    "\n",
    "输出：包含图片中存在的对象及定位框\n",
    "\n",
    "    行人，0 or 1；\n",
    "    汽车，0 or 1；\n",
    "    摩托车，0 or 1；\n",
    "    图片背景，0 or 1；\n",
    "定位框：bx,by,bh,bw\n",
    "其中，bx,by 表示汽车中点，,bh,bw分别表示定位框的高和宽。以图片左上角为(0,0)，以右下角为(1,1)，这些数字均为位置或长度所在图片的比例大小。\n",
    "目标标签 y：\n",
    "![title](img/tagert.png)\n",
    "- 当Pc=1时，表示图片中存在物体；\n",
    "- 当Pc=0时，表示图片中不存在物体，那么此时，输出y的其他值为多少均没有意义，也不会参与损失函数的计算\n",
    "\n",
    "损失函数：\n",
    "如果采用平方误差形式的损失函数：\n",
    "当Pc=1时： \n",
    "L(yhat,y) =(yhat1-y1)**2+(yhat2-y2)**2+(yhat3-y3)**2+...+(yhat8-y8)**2\n",
    "\n",
    "此时，我们需要关注神经网络对所有输出值的准确度；\n",
    "当Pc=0时： \n",
    "L(yhat,y) =(yhat1-y1)**2\n",
    "\n",
    "此时，我们只关注神经网络对背景值的准确度。\n",
    "当然在实际的目标定位应用中，我们可以使用更好的方式是： \n",
    "\n",
    "    * 对c1、c2、c3和softmax使用对数似然损失函数； \n",
    "    * 对边界框的四个值应用平方误差或者类似的方法； \n",
    "    * 对Pc应用logistic regression损失函数，或者平方预测误差。\n",
    "    \n",
    "##### 特征点检测：\n",
    "由前面的目标定位问题，我们可以知道，神经网络可以通过输出图片上特征点的坐标（x,y），来实现对目标特征的识别和定位标记。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标检测\n",
    "目标检测采用的是基于滑动窗口的检测算法。\n",
    "利用滑动窗口在实际图片中实现目标检测。\n",
    "![title](img/目标检测.png)\n",
    "\n",
    "首先选定一个特定大小的窗口，将窗口内的图片输入到模型中进行预测；\n",
    "以固定步幅滑动该窗口，遍历图像的每个区域，对窗内的各个小图不断输入模型进行预测；\n",
    "继续选取一个更大的窗口，再次遍历图像的每个区域，对区域内是否有车进行预测；\n",
    "遍历整个图像，可以保证在每个位置都能检测到是否有车。\n",
    "缺点：计算成本巨大，每个窗口的小图都要进行卷积运算，（但在神经网络兴起之前，使用的是线性分类器，所以滑动窗口算法的计算成本较低）。\n",
    "\n",
    "#### 卷积层替代全连接层：\n",
    "对于卷积网络中全连接层，我们可以利用1×1大小卷积核的卷积层来替代。\n",
    "![title](img/fc2cov.png)\n",
    "1×1的卷积核相当于在一个三维图像的切片上应用了一个全连接的神经网络。同样，全连接层也可以由1×1大小卷积核的卷积层来替代。需注意卷积核的个数与隐层神经元个数相同。\n",
    "\n",
    "汽车目标检测：\n",
    "依据上面的方法，我们将整张图片输入到训练好的卷积神经网络中。无需再利用滑动窗口分割图片，只需一次前向传播，我们就可以同时得到所有图片子集的预测值。\n",
    "利用卷积的方式实现滑动窗口算法的方法，提高了整体的计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box 预测\n",
    "前面一节的卷积方式实现的滑动窗口算法，使得在预测时计算的效率大大提高。但是其存在的问题是：不能输出最精准的边界框（Bounding Box）。\n",
    "在滑动窗口算法中，我们取的一些离散的图片子集的位置，在这种情况下，有可能我们没有得到一个能够完美匹配汽车位置的窗口，也有可能真实汽车的边界框为一个长方形。所以我们需要寻找更加精确的边界框。\n",
    "\n",
    "#### YOLO：\n",
    "YOLO算法可以使得滑动窗口算法寻找到更加精准的边界框\n",
    "- 在整幅图片上加上较为精细的网格，将图片分割成n×n个小的图片；\n",
    "- 采用图像分类和定位算法，分别应用在图像的n×n个格子中。\n",
    "定义训练标签：（对于每个网格，定义如前面的向量yi） yi=[PC bx by bh bw c1 c2 c3]\n",
    "- 对于不同的网格 i有不同的标签向量yi。\n",
    "- 将n×n个格子标签合并在一起，最终的目标输出Y的大小为：n×n×8（这里8是因为例子中的目标值有8个）。\n",
    "\n",
    "通过这样的训练集训练得到目标探测的卷积网络模型。我们利用训练好的模型，将与模型输入相同大小的图片输入到训练好的网络中，得到大小为n×n×8的预测输出。通过观察n×n不同位置的输出值，我们就能知道这些位置中是否存在目标物体，然后也能由存在物体的输出向量得到目标物体的更加精准的边界框。\n",
    "\n",
    "##### YOLO notation：\n",
    "\n",
    "    将对象分配到一个格子的过程是：观察对象的中点，将该对象分配到其中点所在的格子中，（即使对象横跨多个格子，也只分配到中点所在的格子中，其他格子记为无该对象，即标记为“0”）；\n",
    "    YOLO显式地输出边界框，使得其可以具有任意宽高比，并且能输出更精确的坐标，不受滑动窗口算法滑动步幅大小的限制；\n",
    "    YOLO是一次卷积实现，并不是在n×n网格上进行n2次运算，而是单次卷积实现，算法实现效率高，运行速度快，可以实现实时识别。\n",
    "    \n",
    "##### bounding boxes 细节：\n",
    "利用YOLO算法实现目标探测的时候，对于存在目标对象的网格中，定义训练标签Y的时候，边界框的指定参数的不同对其预测精度有很大的影响。这里给出一个较为合理的约定：（其他定值方式可阅读论文）\n",
    "\n",
    "    对于每个网格，以左上角为(0,0)，以右下角为(1,1)；\n",
    "    中点bx、by表示坐标值，在0~1之间；\n",
    "    宽高bh、bw表示比例值，存在>1的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交并比（Intersection-over-Union）\n",
    "交并比函数用来评价目标检测算法是否运作良好。\n",
    "对于理想的边界框和目标探测算法预测得到的边界框，交并比函数计算两个边界框交集和并集之比。\n",
    "一般在目标检测任务中，约定如果 IoU⩾0.5 ，那么就说明检测正确。当然标准越大，则对目标检测算法越严格。得到的IoU值越大越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非最大值抑制（non-max suppression，NMS）\n",
    "对于我们前面提到的目标检测算法，可能会对同一个对象做出多次的检测，非最大值抑制可以确保我们的算法对每个对象只检测一次。\n",
    "\n",
    "多网格检测同一物体：\n",
    "对于汽车目标检测的例子中，我们将图片分成很多精细的格子。最终预测输出的结果中，可能会有相邻的多个格子里均检测出都具有同一个对象。\n",
    "\n",
    "NMS算法思想：\n",
    "\n",
    "    在对n×n个网格进行目标检测算法后，每个网格输出的Pc为一个0~1的值，表示有车的概率大小。其中会有多个网格内存在高概率；\n",
    "    得到对同一个对象的多次检测，也就是在一个对象上有多个具有重叠的不同的边界框；\n",
    "    非最大值抑制对多种检测结果进行清理：选取最大Pc的边界框，对所有其他与该边界框具有高交并比或高重叠的边界框进行抑制；\n",
    "    逐一审视剩下的边界框，寻找最高的Pc值边界框，重复上面的步骤。\n",
    "    非最大值抑制，也就是说抑制那些不是最大值，却比较接近最大值的边界框。\n",
    "    \n",
    "    \n",
    "NMS算法：\n",
    "\n",
    "    以单个对象检测为例：\n",
    "    对于图片每个网格预测输出矩阵：[Pc bx by bh bw ]\n",
    "    抛弃Pc⩽0.6的边界框；\n",
    "    对剩余的边界框（while）： \n",
    "        选取最大Pc值的边界框，作为预测输出边界框；\n",
    "        抛弃和选取的边界框IoU⩾0.5的剩余的边界框。\n",
    "    对于多对象检测，输出标签中就会有多个分量。正确的做法是：对每个输出类别分别独立进行一次非最大值抑制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor box\n",
    "通过上面的各种方法，目前我们的目标检测算法在每个格子上只能检测出一个对象。使用Anchor box 可以同时检测出多个对象。\n",
    "\n",
    "对于重叠的目标，这些目标的中点有可能会落在同一个网格中，对于我们之前定义的输出：yi=[PC bx by bh bw c1 c2 c3]只能得到一个目标的输出。\n",
    "\n",
    "而Anchor box 则是预先定义多个不同形状的Anchor box，我们需要把预测目标对应地和各个Anchor box 关联起来，所以我们重新定义目标向量： \n",
    "yi=[PC bx by bh bw c1 c2 c3 PC bx by bh bw c1 c2 c3]\n",
    "用这样的多目标向量分别对应不同的Anchor box，从而检测出多个重叠的目标。\n",
    "\n",
    "不使用Anchor box：训练图片中的每个对象，根据对象的中点，分配到对应的格子中。输出大小（例如8）：n×n×8；\n",
    "\n",
    "使用Anchor box：训练图片的每个对象，根据对象的中点，分配到对应的格子中，同时还分配到一个和对象形状的IoU最高的Anchor box 中。输出大小（例如两个Anchor box）：n×n×16。\n",
    "\n",
    "难点问题：\n",
    "\n",
    "    如果我们使用了两个Anchor box，但是同一个格子中却有三个对象的情况，此时只能用一些额外的手段来处理；\n",
    "    同一个格子中存在两个对象，但它们的Anchor box 形状相同，此时也需要引入一些专门处理该情况的手段。\n",
    "    但是以上的两种问题出现的可能性不会很大，对目标检测算法不会带来很大的影响。\n",
    "    \n",
    "Anchor box 的选择：\n",
    "\n",
    "    一般人工指定Anchor box 的形状，选择5~10个以覆盖到多种不同的形状，可以涵盖我们想要检测的对象的形状；\n",
    "    高级方法：K-means 算法：将不同对象形状进行聚类，用聚类后的结果来选择一组最具代表性的Anchor box，以此来代表我们想要检测对象的形状。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO算法目标检测\n",
    "假设我们要在图片中检测三种目标：行人、汽车和摩托车，同时使用两种不同的Anchor box。\n",
    "#### 训练集：\n",
    "\n",
    "    输入X：同样大小的完整图片；\n",
    "    目标Y：使用3×3网格划分，输出大小3×3×2×8，或者3×3×16\n",
    "    对不同格子中的小图，定义目标输出向量Y。\n",
    "    \n",
    "#### 模型预测：\n",
    "\n",
    "输入与训练集中相同大小的图片，同时得到每个格子中不同的输出结果：3×3×2×8。  \n",
    "\n",
    "#### 运行非最大值抑制（NMS）：\n",
    "\n",
    "- 假设使用了2个Anchor box，那么对于每一个网格，我们都会得到预测输出的2个bounding boxes，其中一个Pc比较高； \n",
    "- 抛弃概率Pc值低的预测bounding boxes；\n",
    "- 对每个对象（如行人、汽车、摩托车）分别使用NMS算法得到最终的预测边界框。 \n",
    "- 候选区域（region proposals）\n",
    "\n",
    "\n",
    "#### R-CNN：\n",
    "R-CNN（Regions with convolutional networks），会在我们的图片中选出一些目标的候选区域，从而避免了传统滑动窗口在大量无对象区域的无用运算。\n",
    "所以在使用了R-CNN后，我们不会再针对每个滑动窗口运算检测算法，而是只选择一些候选区域的窗口，在少数的窗口上运行卷积网络。\n",
    "具体实现：运用图像分割算法，将图片分割成许多不同颜色的色块，然后在这些色块上放置窗口，将窗口中的内容输入网络，从而减小需要处理的窗口数量。\n",
    "\n",
    "\n",
    "#### 更快的算法：\n",
    "\n",
    "    R-CNN：给出候选区域，对每个候选区域进行分类识别，输出对象 标签 和 bounding box，从而在确实存在对象的区域得到更精确的边界框，但速度慢；\n",
    "    Fast R-CNN：给出候选区域，使用滑动窗口的卷积实现去分类所有的候选区域，但得到候选区的聚类步骤仍然非常慢；\n",
    "    Faster R-CNN：使用卷积网络给出候选区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
